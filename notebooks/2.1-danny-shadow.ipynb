{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os.path as osp\n",
    "\n",
    "DATA_DIR = '/home/gangda/workspace/graph_engine/intermediate'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_59543/1035705415.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m         \u001B[0mres\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mosp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDATA_DIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'data_{}_{}.pt'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[1;32m    787\u001B[0m                     \u001B[0;32mexcept\u001B[0m \u001B[0mRuntimeError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    788\u001B[0m                         \u001B[0;32mraise\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mUnpicklingError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mUNSAFE_MESSAGE\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 789\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0m_load\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_zipfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpickle_module\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    790\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mweights_only\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    791\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_load\u001B[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1129\u001B[0m     \u001B[0munpickler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mUnpicklerWrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mpickle_load_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1130\u001B[0m     \u001B[0munpickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpersistent_load\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpersistent_load\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1131\u001B[0;31m     \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0munpickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1133\u001B[0m     \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_loaded_sparse_tensors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_utils.py\u001B[0m in \u001B[0;36m_rebuild_tensor_v2\u001B[0;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m def _rebuild_tensor_v2(\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0mstorage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstorage_offset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequires_grad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbackward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m ):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(2):\n",
    "    for j in range(8):\n",
    "        res += torch.load(osp.join(DATA_DIR, 'data_{}_{}.pt'.format(i, j)))\n",
    "\n",
    "len(res)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import os\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "file_path = '/home/gangda/workspace/graph_engine/data/ogbn-products-p2'\n",
    "\n",
    "def extract_core_global_ids(parts):\n",
    "    part_core_global_ids = []\n",
    "    for i in range(len(parts)):\n",
    "        part = parts[i]\n",
    "        core_mask = part.ndata['inner_node'].type(torch.bool)\n",
    "        part_global_id = part.ndata['orig_id']\n",
    "        part_core_global_ids.append(part_global_id[core_mask])\n",
    "    return part_core_global_ids\n",
    "\n",
    "og, y = DglNodePropPredDataset(name='ogbn-products', root='/data/gangda/dgl')[0]\n",
    "dataset = dict(X=og.ndata['feat'], y=y)\n",
    "dataset['edge_index'] = torch.load(os.path.join(file_path, 'dgl_edge_index.pt'))\n",
    "parts = torch.load(os.path.join(file_path, 'metis_partitions.pt'))\n",
    "dataset['part_core_global_ids'] = extract_core_global_ids(parts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# [1235633, 1213396]\n",
    "vids = torch.cat([torch.arange(1235633), torch.arange(1213396)])\n",
    "sids = torch.cat([torch.zeros(1235633), torch.ones(1213396)])\n",
    "\n",
    "def get_global_id(local_ids, shard_ids):\n",
    "    local_ids = local_ids.to(torch.long)\n",
    "    global_ids = torch.empty_like(local_ids)\n",
    "    for j in range(len(dataset['part_core_global_ids'])):\n",
    "        mask = shard_ids == j\n",
    "        if mask.sum() == 0: continue\n",
    "        global_ids[mask] = dataset['part_core_global_ids'][j][local_ids[mask]]\n",
    "    return global_ids\n",
    "\n",
    "gids = get_global_id(vids, sids)\n",
    "gids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "2449029"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 150\n",
    "\n",
    "global_topk = []\n",
    "for i, r in enumerate(res):\n",
    "    val, idx = torch.sort(r[2], descending=True)\n",
    "    top_k_index = idx[:K]\n",
    "    global_ids = get_global_id(r[0][top_k_index], r[1][top_k_index])\n",
    "    vec = torch.full((K,), gids[i])\n",
    "    vec[:global_ids.shape[0]] = global_ids\n",
    "    global_topk.append(vec)\n",
    "global_topk = torch.stack(global_topk)\n",
    "\n",
    "ppr_matrix = torch.empty((dataset['X'].shape[0], K), dtype=torch.long)\n",
    "ppr_matrix[gids] = global_topk\n",
    "ppr_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 769608,  228252,  960517,  132355,       0, 1043730,    4467,  657230,\n          33678,  188057])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(ppr_matrix, osp.join(DATA_DIR, 'ppr_matrix.pt'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[      0,  152857,  194591,  ...,  186457, 2034637,   35148],\n        [      1,   89825,  151342,  ..., 1137752,  620866, 2322836],\n        [      2,  488076, 1598820,  ..., 2017855,  442068, 1514525],\n        ...,\n        [2449026,  149963,  148503,  ..., 1886107, 2210439, 2009035],\n        [2449027,  739621,  306502,  ...,  188674,  307615,  706682],\n        [2449028,  728426,   48117,  ..., 1815472, 1420498, 2088456]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppr_matrix = torch.load(osp.join(DATA_DIR, 'ogbn-products_ppr_matrix.pt'))\n",
    "ppr_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035,\n        1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047,\n        1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059,\n        1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071,\n        1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083,\n        1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095,\n        1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107,\n        1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119,\n        1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131,\n        1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143,\n        1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_index = torch.arange(1024, 1024 + 128)\n",
    "batch_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "from pyg_lib.sampler import subgraph as libsubgraph\n",
    "\n",
    "adj = SparseTensor.from_edge_index(dataset['edge_index'])\n",
    "rowptr, col, _ = adj.csr()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([     0,     96,    121,  ..., 545857, 545883, 545884]),\n tensor([  407,   520,   582,  ..., 16622, 17125,   121]),\n tensor([    10188,     10189,     10190,  ..., 123715627, 123715631,\n         123716018]))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = torch.cat([batch_index, ppr_matrix[batch_index].view(-1)])\n",
    "subset, inv = subset.unique(return_inverse=True)\n",
    "\n",
    "libsubgraph(rowptr, col, subset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.18 s, sys: 342 ms, total: 8.52 s\n",
      "Wall time: 64.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "Data(x=[18918, 100], edge_index=[18918, 18918, nnz=545884], y=[128], ego_index=[128])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from torch_geometric.utils import subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "subset = torch.cat([batch_index, ppr_matrix[batch_index].view(-1)])\n",
    "subset, inv = subset.unique(return_inverse=True)\n",
    "\n",
    "sub_rowptr, sub_col, _ = libsubgraph(rowptr, col, subset)\n",
    "adj = SparseTensor(rowptr=sub_rowptr, col=sub_col)\n",
    "\n",
    "# sub_edge_index = subgraph(subset, dataset['edge_index'])[0]\n",
    "# sub_edge_index = sub_edge_index.unique(return_inverse=True)[1]\n",
    "\n",
    "batch_data = Data(dataset['X'][subset],\n",
    "                  edge_index=adj,\n",
    "                  y=dataset['y'][batch_index].view(-1),\n",
    "                  ego_index=inv[:batch_index.shape[0]])\n",
    "batch_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 7s, sys: 0 ns, total: 8min 7s\n",
      "Wall time: 31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataBatch(x=[11491, 100], edge_index=[2, 158175], y=[128], ego_index=[128], batch=[11491], ptr=[129])"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "datas = []\n",
    "for bid in batch_index:\n",
    "    subset, inv = torch.cat([bid.unsqueeze(dim=0), ppr_matrix[bid]]).unique(return_inverse=True)\n",
    "    ego_index = inv[0]\n",
    "    # subset = ppr_matrix[bid].unique()\n",
    "    # ego_index=torch.tensor([0])\n",
    "\n",
    "    edge_id = libsubgraph(rowptr, col, subset, return_edge_id=True)[-1]\n",
    "    sub_edge_index = dataset['edge_index'][:, edge_id]\n",
    "    # sub_edge_index = subgraph(ppr_matrix[bid], dataset['edge_index'])[0]\n",
    "    sub_edge_index = sub_edge_index.unique(return_inverse=True)[1]\n",
    "\n",
    "    ego_data = Data(dataset['X'][subset],\n",
    "                    sub_edge_index,\n",
    "                    y=dataset['y'][bid],\n",
    "                    ego_index=ego_index\n",
    "                    )\n",
    "    datas.append(ego_data)\n",
    "\n",
    "batch = Batch.from_data_list(datas)\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Complete!\n"
     ]
    }
   ],
   "source": [
    "d = DglNodePropPredDataset(name='ogbn-products', root='/data/gangda/dgl')\n",
    "og, y = d[0]\n",
    "dataset = dict(X=og.ndata['feat'], y=y)\n",
    "dataset['train_index'] = d.get_idx_split()['train']\n",
    "dataset['valid_index'] = d.get_idx_split()['valid']\n",
    "dataset['test_index'] = d.get_idx_split()['test']\n",
    "\n",
    "data_list = torch.load('/home/gangda/workspace/graph_engine/intermediate/egograph_list.pt')\n",
    "print('Data Loading Complete!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "class Dict(dict):\n",
    "    def __getattr__(self, key):\n",
    "        return self.get(key)\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "args = Dict({\n",
    "    'batch_size': 128,\n",
    "    'runs': 5,\n",
    "    'epochs': 35,\n",
    "    'lr': 0.001,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import subgraph, dropout_edge\n",
    "from torchmetrics import Accuracy\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.nn import GATConv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4,\n",
    "                 num_layers=5, dropout=0.35, dropedge=0.1, pooling='center'):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.dropedge = dropedge\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GATConv(in_channels, hidden_channels, heads))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(heads * hidden_channels, hidden_channels, heads))\n",
    "        self.convs.append(GATConv(heads * hidden_channels, out_channels, heads))\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor, ego_index: Tensor) -> Tensor:\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = conv(x, dropout_edge(edge_index, p=self.dropedge, training=self.training)[0])\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = x.relu_()\n",
    "\n",
    "        if self.pooling == 'center':\n",
    "            x = x[ego_index]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ShadowLoader(DataLoader):\n",
    "    def __init__(self, node_idx, data_list, **kwargs):\n",
    "        self.data_list = data_list\n",
    "        if node_idx.dtype == torch.bool:\n",
    "            node_idx = node_idx.nonzero(as_tuple=False).view(-1)\n",
    "        super().__init__(node_idx.tolist(), collate_fn=self.__collate__, **kwargs)\n",
    "\n",
    "    def __collate__(self, batch_nodes):\n",
    "        batch_data_list = []\n",
    "        for nid in batch_nodes:\n",
    "            batch_data_list.append(self.data_list[nid])\n",
    "        return Batch.from_data_list(batch_data_list)\n",
    "\n",
    "\n",
    "def train(model, optimizer, metric, train_loader, epoch):\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "    pbar = tqdm(total=int(len(train_loader.dataset)))\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch.to(metric.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(batch.x, batch.edge_index, batch.ego_index)\n",
    "        loss = F.cross_entropy(y_hat, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() / batch.y.shape[0]\n",
    "        metric.update(y_hat.argmax(dim=-1), batch.y)\n",
    "        pbar.update(batch.y.shape[0])\n",
    "    pbar.close()\n",
    "\n",
    "    return total_loss, metric.compute()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mini_test(model, metric, *loaders):\n",
    "    model.eval()\n",
    "    ms = []\n",
    "    for loader in loaders:\n",
    "        metric.reset()\n",
    "        for data in tqdm(loader):\n",
    "            data.to(metric.device)\n",
    "            y_hat = model(data.x, data.edge_index, data.ego_index)\n",
    "            metric.update(y_hat.argmax(dim=-1), data.y)\n",
    "        ms.append(metric.compute())\n",
    "    return ms\n",
    "\n",
    "\n",
    "def main(args, dataset, data_list):\n",
    "    num_features, num_classes = dataset['X'].shape[-1], dataset['y'].max().item()+1\n",
    "\n",
    "    kwargs = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 1, 'persistent_workers': True}\n",
    "    train_loader = ShadowLoader(dataset['train_index'], data_list, **kwargs)\n",
    "    val_loader = ShadowLoader(dataset['valid_index'], data_list, **kwargs)\n",
    "    test_loader = ShadowLoader(dataset['test_index'], data_list, **kwargs)\n",
    "\n",
    "    device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "    metric = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "    metric.to(device)\n",
    "\n",
    "    # runs\n",
    "    best_val, best_test = [], []\n",
    "    for i in range(1, args.runs + 1):\n",
    "        model = GAT(num_features, 256, num_classes).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=0)\n",
    "\n",
    "        best_val_acc = test_acc = 0\n",
    "        test_accs = []\n",
    "        print(f'------------------------{i}------------------------')\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            loss, train_acc = train(model, optimizer, metric, train_loader, epoch)\n",
    "            print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {train_acc:.4f}')\n",
    "            if epoch > 20 and epoch % 5 == 0:\n",
    "                val_acc, tmp_test_acc = mini_test(model, metric, val_loader, test_loader)\n",
    "                test_accs.append(round(tmp_test_acc.item(), 3))\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    test_acc = tmp_test_acc\n",
    "                print(f'Epoch: {epoch:02d}, Loss: {loss: .4f}, Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')\n",
    "\n",
    "        best_val.append(float(best_val_acc))\n",
    "        best_test.append(float(test_acc))\n",
    "        print(test_accs)\n",
    "\n",
    "    print(f'Valid: {np.mean(best_val):.4f} +- {np.std(best_val):.4f}')\n",
    "    print(f'Test: {np.mean(best_test):.4f} +- {np.std(best_test):.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------1------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 01:   0%|          | 128/196615 [00:02<54:42, 59.86it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f2604071ca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gangda/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/gangda/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/gangda/anaconda3/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f2604071ca0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gangda/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/gangda/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/gangda/anaconda3/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Epoch 01: 100%|██████████| 196615/196615 [02:24<00:00, 1357.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Loss: 6.2611, Approx. Train: 0.8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 02: 100%|██████████| 196615/196615 [02:22<00:00, 1382.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02, Loss: 4.2208, Approx. Train: 0.9046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 03: 100%|██████████| 196615/196615 [02:22<00:00, 1383.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03, Loss: 3.7545, Approx. Train: 0.9132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 04: 100%|██████████| 196615/196615 [02:22<00:00, 1377.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04, Loss: 3.3817, Approx. Train: 0.9203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 05: 100%|██████████| 196615/196615 [02:22<00:00, 1379.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05, Loss: 3.7369, Approx. Train: 0.9154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 06: 100%|██████████| 196615/196615 [02:22<00:00, 1380.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06, Loss: 3.4559, Approx. Train: 0.9204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 07: 100%|██████████| 196615/196615 [02:22<00:00, 1379.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07, Loss: 3.3202, Approx. Train: 0.9216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 08: 100%|██████████| 196615/196615 [02:22<00:00, 1376.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08, Loss: 3.8664, Approx. Train: 0.9145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 09: 100%|██████████| 196615/196615 [02:22<00:00, 1383.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09, Loss: 3.7716, Approx. Train: 0.9113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 196615/196615 [02:22<00:00, 1380.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 3.7703, Approx. Train: 0.9107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:  51%|█████     | 100480/196615 [01:12<01:13, 1314.27it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main(args, dataset, data_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "kwargs = {'batch_size': args.batch_size, 'shuffle': True, 'num_workers': 1, 'persistent_workers': True}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "train_loader = ShadowLoader(dataset['train_index'], data_list, **kwargs)\n",
    "iter = train_loader.__iter__()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.9 ms, sys: 50 µs, total: 3.95 ms\n",
      "Wall time: 3.97 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataBatch(x=[11139, 100], edge_index=[2, 149450], y=[128], ego_index=[128], batch=[11139], ptr=[129])"
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "batch = next(iter)\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "data = PygNodePropPredDataset('ogbn-products', root='/data/gangda/ogb')[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import torch\n",
    "data.n_id = torch.arange(data.num_nodes)\n",
    "subgraph_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=None,\n",
    "    num_neighbors=[-1],\n",
    "    batch_size=4096,\n",
    "    num_workers=1,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "it = iter(subgraph_loader)\n",
    "batch = next(it)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([     0,      1,      2,  ..., 113059, 380489, 634640])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.n_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Graph(num_nodes=2708, num_edges=10556,\n      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool)}\n      edata_schemes={})"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "og = CoraGraphDataset(raw_dir='/data/gangda/dgl', verbose=False)[0]\n",
    "og"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(6)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og.ndata['label'].max()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
